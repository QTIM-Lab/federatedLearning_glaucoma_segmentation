{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A\n",
    "from transformers import MaskFormerImageProcessor, Mask2FormerForUniversalSegmentation\n",
    "import evaluate\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from custom_datasets import ImageSegmentationDataset\n",
    "from utils import color_palette\n",
    "import copy\n",
    "\n",
    "# def parse_args():\n",
    "#     parser = argparse.ArgumentParser(description=\"Train MaskFormer model for instance segmentation\")\n",
    "#     parser.add_argument(\"--train_csv\", type=str, default='./outputs', help=\"Path to .csv file with rows for all train\")\n",
    "#     parser.add_argument(\"--val_csv\", type=str, default='./outputs', help=\"Path to .csv file with rows for all val\")\n",
    "#     parser.add_argument(\"--csv_img_path_col\", type=str, default='image', help=\"Column name in the csv for the path to the image\")\n",
    "#     parser.add_argument(\"--csv_label_path_col\", type=str, default='label', help=\"Column name in the csv for the path to the segmentation label\")\n",
    "#     parser.add_argument(\"--output_directory\", type=str, default='./outputs', help=\"Desired path for output files (model, val inferences, etc)\")\n",
    "#     parser.add_argument('--dataset_mean', nargs='+', type=float, help='Array of float values for mean i.e. 0.709 0.439 0.287')\n",
    "#     parser.add_argument('--dataset_std', nargs='+', type=float, help='Array of float values for std i.e. 0.210 0.220 0.199')\n",
    "#     parser.add_argument(\"--lr\", type=float, default=0.00003, help=\"Learning rate for the optimizer\")\n",
    "#     parser.add_argument(\"--batch_size\", type=int, default=16, help=\"Batch size for training and testing\")\n",
    "#     parser.add_argument('--jitters', nargs='+', type=float, help='Array of float jitter values: brightness, contrast, saturation, hue, probability')\n",
    "#     parser.add_argument(\"--num_epochs\", type=int, default=50, help=\"Max number of epochs to train\")\n",
    "#     parser.add_argument(\"--patience\", type=int, default=5, help=\"Early stopping\")\n",
    "#     parser.add_argument(\"--num_val_outputs_to_save\", type=int, default=3, help=\"Number of examples from val to save, so you can see your model improve on it during training.\")\n",
    "#     parser.add_argument(\"--num_workers\", type=int, default=0, help=\"Number of workers for dataloaders\")\n",
    "#     return parser.parse_args()\n",
    "\n",
    "# Federated Learning Datasets\n",
    "dataset_dirs = [\n",
    "    \"/sddata/data/retina_datasets_preprocessed/federated_learning_public/binrushed\",\n",
    "    \"/sddata/data/retina_datasets_preprocessed/federated_learning_public/drishti\",\n",
    "    \"/sddata/data/retina_datasets_preprocessed/federated_learning_public/magrabi\"\n",
    "]\n",
    "\n",
    "def load_dataset(dataset_dir, transform):\n",
    "    image_dir = os.path.join(dataset_dir, \"images\")\n",
    "    label_dir = os.path.join(dataset_dir, \"labels\")\n",
    "    image_paths = [os.path.join(image_dir, img) for img in os.listdir(image_dir)]\n",
    "    label_paths = [os.path.join(label_dir, lbl) for lbl in os.listdir(label_dir)]\n",
    "    return ImageSegmentationDataset(image_paths, label_paths, transform=transform)\n",
    "\n",
    "def FedAvg(weights):\n",
    "    global_model = copy.deepcopy(weights[0])\n",
    "    for key in global_model.keys():\n",
    "        for i in range(1, len(weights)):\n",
    "            global_model[key] += weights[i][key]\n",
    "        global_model[key] = torch.div(global_model[key], len(weights))\n",
    "    return global_model\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        images, labels = batch\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def main():\n",
    "    # args = parse_args()\n",
    "    # ipynb values \n",
    "    train_csv = '/home/thakuriu/fl_glaucoma_seg/csvs/binrushed_train.csv' \n",
    "    val_csv = '/home/thakuriu/fl_glaucoma_seg/csvs/binrushed_val.csv' \n",
    "    csv_img_path_col  = 'image_path'\n",
    "    csv_label_path_col  = 'label_path'\n",
    "    output_directory = '/home/thakuriu/fl_glaucoma_seg/detection_segmentation_v2/segmentation_train_and_inference/train_outputs'\n",
    "    dataset_mean=[0.768, 0.476, 0.289]\n",
    "    dataset_std = [0.221, 0.198, 0.165]\n",
    "    lr = 0.00003 \n",
    "    batch_size = 8 \n",
    "    jitters = [0.2, 0.2, 0.05, 0.05, 0.75] \n",
    "    num_epochs = 100 \n",
    "    patience = 7 \n",
    "    num_val_outputs_to_save = 5 \n",
    "    num_workers = 0 # 16\n",
    "\n",
    "    device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # [Your existing code for initializing output folders, transforms, etc.]\n",
    "\n",
    "    # Prepare datasets and dataloaders\n",
    "    datasets = [load_dataset(dir, train_transform) for dir in dataset_dirs]\n",
    "    dataloaders = [DataLoader(ds, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers) for ds in datasets]\n",
    "\n",
    "    # Initialize models, optimizers, and criterion\n",
    "    models = [Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-large-cityscapes-semantic\",\n",
    "                                                                 id2label=id2label,\n",
    "                                                                 ignore_mismatched_sizes=True).to(device) for _ in dataset_dirs]\n",
    "    optimizers = [optim.AdamW(model.parameters(), lr=lr) for model in models]\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        weights = []\n",
    "\n",
    "        # Train each model on its dataset\n",
    "        for i, (model, dataloader, optimizer) in enumerate(zip(models, dataloaders, optimizers)):\n",
    "            loss = train_one_epoch(model, dataloader, optimizer, criterion, device)\n",
    "            print(f\"Training Loss for Dataset {i+1}: {loss}\")\n",
    "            weights.append(model.state_dict())\n",
    "\n",
    "        # Federated averaging\n",
    "        global_weights = FedAvg(weights)\n",
    "        for model in models:\n",
    "            model.load_state_dict(global_weights)\n",
    "\n",
    "        # Validation code can be added here\n",
    "\n",
    "    # Save the global model\n",
    "    torch.save(global_weights, os.path.join(model_directory, 'global_model.pth'))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
