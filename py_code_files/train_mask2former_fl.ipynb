{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import copy\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A\n",
    "from transformers import MaskFormerImageProcessor, Mask2FormerForUniversalSegmentation\n",
    "import evaluate\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from custom_datasets import ImageSegmentationDataset\n",
    "from utils import color_palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_args():\n",
    "#     parser = argparse.ArgumentParser(description=\"Train MaskFormer model for instance segmentation\")\n",
    "#     parser.add_argument(\"--train_csv\", type=str, default='./outputs', help=\"Path to .csv file with rows for all train\")\n",
    "#     parser.add_argument(\"--val_csv\", type=str, default='./outputs', help=\"Path to .csv file with rows for all val\")\n",
    "#     parser.add_argument(\"--csv_img_path_col\", type=str, default='image', help=\"Column name in the csv for the path to the image\")\n",
    "#     parser.add_argument(\"--csv_label_path_col\", type=str, default='label', help=\"Column name in the csv for the path to the segmentation label\")\n",
    "#     parser.add_argument(\"--output_directory\", type=str, default='./outputs', help=\"Desired path for output files (model, val inferences, etc)\")\n",
    "#     parser.add_argument('--dataset_mean', nargs='+', type=float, help='Array of float values for mean i.e. 0.709 0.439 0.287')\n",
    "#     parser.add_argument('--dataset_std', nargs='+', type=float, help='Array of float values for std i.e. 0.210 0.220 0.199')\n",
    "#     parser.add_argument(\"--lr\", type=float, default=0.00003, help=\"Learning rate for the optimizer\")\n",
    "#     parser.add_argument(\"--batch_size\", type=int, default=16, help=\"Batch size for training and testing\")\n",
    "#     parser.add_argument('--jitters', nargs='+', type=float, help='Array of float jitter values: brightness, contrast, saturation, hue, probability')\n",
    "#     parser.add_argument(\"--num_epochs\", type=int, default=50, help=\"Max number of epochs to train\")\n",
    "#     parser.add_argument(\"--patience\", type=int, default=5, help=\"Early stopping\")\n",
    "#     parser.add_argument(\"--num_val_outputs_to_save\", type=int, default=3, help=\"Number of examples from val to save, so you can see your model improve on it during training.\")\n",
    "#     parser.add_argument(\"--num_workers\", type=int, default=0, help=\"Number of workers for dataloaders\")\n",
    "#     return parser.parse_args()\n",
    "\n",
    "# Federated Average Function\n",
    "def FedAvg(models):\n",
    "    global_model = copy.deepcopy(models[0])\n",
    "\n",
    "    for key in global_model.state_dict().keys():\n",
    "        global_model.state_dict()[key] = torch.stack([model.state_dict()[key].float() for model in models], 0).mean(0)\n",
    "    \n",
    "    return global_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python train_mask2former.py \\\n",
    "    # --train_csv /sddata/data/retina_datasets_preprocessed/csvs/combined_train.csv \\\n",
    "    # --val_csv /sddata/data/retina_datasets_preprocessed/csvs/combined_val.csv \\\n",
    "    # --csv_img_path_col image_path \\\n",
    "    # --csv_label_path_col label_path \\\n",
    "    # --output_directory /home/thakuriu/fl_glaucoma_seg/detection_segmentation_v2/segmentation_train_and_inference/train_outputs \\\n",
    "    # --dataset_mean 0.768 0.476 0.289 \\\n",
    "    # --dataset_std 0.221 0.198 0.165 \\\n",
    "    # --lr 0.00003 \\\n",
    "    # --batch_size 8 \\\n",
    "    # --jitters 0.2 0.2 0.05 0.05 0.75 \\\n",
    "    # --num_epochs 100 \\\n",
    "    # --patience 7 \\\n",
    "    # --num_val_outputs_to_save 5 \\\n",
    "    # --num_workers 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device:  cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thakuriu/.local/lib/python3.10/site-packages/transformers/models/maskformer/image_processing_maskformer.py:428: FutureWarning: The `reduce_labels` argument is deprecated and will be removed in v4.27. Please use `do_reduce_labels` instead.\n",
      "  warnings.warn(\n",
      "Some weights of Mask2FormerForUniversalSegmentation were not initialized from the model checkpoint at facebook/mask2former-swin-large-cityscapes-semantic and are newly initialized because the shapes did not match:\n",
      "- class_predictor.bias: found shape torch.Size([20]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "- class_predictor.weight: found shape torch.Size([20, 256]) in the checkpoint and torch.Size([5, 256]) in the model instantiated\n",
      "- criterion.empty_weight: found shape torch.Size([20]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 275\u001b[0m\n\u001b[1;32m    271\u001b[0m     plt\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 275\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 125\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Replaces the head of the pre-trained model\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;66;03m# model = MaskFormerForInstanceSegmentation.from_pretrained(\"facebook/maskformer-swin-base-ade\",\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m#                                                         id2label=id2label,\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m#                                                         ignore_mismatched_sizes=True)\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     model \u001b[38;5;241m=\u001b[39m Mask2FormerForUniversalSegmentation\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/mask2former-swin-large-cityscapes-semantic\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    123\u001b[0m                                                                 id2label\u001b[38;5;241m=\u001b[39mid2label,\n\u001b[1;32m    124\u001b[0m                                                                 ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 125\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;66;03m# experiment id should be a unique string value for the experiment. can use the output folders name\u001b[39;00m\n\u001b[1;32m    128\u001b[0m     metric \u001b[38;5;241m=\u001b[39m evaluate\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_iou\u001b[39m\u001b[38;5;124m\"\u001b[39m, experiment_id\u001b[38;5;241m=\u001b[39moutput_directory\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime()))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:2271\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2267\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2268\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2269\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2270\u001b[0m         )\n\u001b[0;32m-> 2271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 810 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # args = parse_args()\n",
    "    # train_csv = args.train_csv\n",
    "    # val_csv = args.val_csv\n",
    "    # csv_img_path_col = args.csv_img_path_col\n",
    "    # csv_label_path_col = args.csv_label_path_col\n",
    "    # output_directory = args.output_directory\n",
    "    # dataset_mean = args.dataset_mean\n",
    "    # dataset_std = args.dataset_std\n",
    "    # lr = args.lr\n",
    "    # batch_size = args.batch_size\n",
    "    # jitters = args.jitters\n",
    "    # num_epochs = args.num_epochs\n",
    "    # patience = args.patience\n",
    "    # num_val_outputs_to_save = args.num_val_outputs_to_save\n",
    "    # num_workers = args.num_workers\n",
    "    \n",
    "    # assert len(jitters) == 5, 'jitters must have 5 values'\n",
    "    # assert (jitters[0] < 1 and jitters[0] > 0 \n",
    "    #         and jitters[1] <= 1 and jitters[1] >= 0 \n",
    "    #         and jitters[2] <= 1 and jitters[2] >= 0 \n",
    "    #         and jitters[3] <= 1 and jitters[3] >= 0\n",
    "    #         and jitters[4] <= 1 and jitters[4] >= 0), 'jitters must be [0,1]'\n",
    "    # assert len(dataset_mean) == 3, 'dataset mean must have 3 float values'\n",
    "    # assert len(dataset_std) == 3, 'dataset std must have 3 float values'\n",
    "    \n",
    "    # ipynb stuff:\n",
    "    train_csv = '/home/thakuriu/fl_glaucoma_seg/csvs/binrushed_train.csv' \n",
    "    val_csv = '/home/thakuriu/fl_glaucoma_seg/csvs/binrushed_val.csv' \n",
    "    csv_img_path_col  = 'image_path'\n",
    "    csv_label_path_col  = 'label_path'\n",
    "    output_directory = '/home/thakuriu/fl_glaucoma_seg/detection_segmentation_v2/segmentation_train_and_inference/train_outputs'\n",
    "    dataset_mean=[0.768, 0.476, 0.289]\n",
    "    dataset_std = [0.221, 0.198, 0.165]\n",
    "    lr = 0.00003 \n",
    "    batch_size = 8 \n",
    "    jitters = [0.2, 0.2, 0.05, 0.05, 0.75] \n",
    "    num_epochs = 100 \n",
    "    patience = 7 \n",
    "    num_val_outputs_to_save = 5 \n",
    "    num_workers = 0 # 16\n",
    "    \n",
    "    \n",
    "\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('using device: ', device)\n",
    "\n",
    "    ### Initialize output folders for the model, val inferences\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    model_directory = os.path.join(output_directory, 'model')\n",
    "    os.makedirs(model_directory, exist_ok=True)\n",
    "\n",
    "    inference_directory = os.path.join(output_directory, 'inference')\n",
    "    os.makedirs(inference_directory, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    id2label = {\n",
    "        0: \"unlabeled\",\n",
    "        1: \"bg\",\n",
    "        2: \"disc\",\n",
    "        3: \"cup\"\n",
    "    }\n",
    "\n",
    "    # for vis\n",
    "    palette = color_palette()\n",
    "\n",
    "    # transforms\n",
    "    ADE_MEAN = np.array(dataset_mean)\n",
    "    ADE_STD = np.array(dataset_std)\n",
    "\n",
    "    train_transform = A.Compose([\n",
    "        A.ColorJitter(brightness=jitters[0], contrast=jitters[1], saturation=jitters[2], hue=jitters[3], p=jitters[4]),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Normalize(mean=ADE_MEAN, std=ADE_STD),\n",
    "    ])\n",
    "\n",
    "    val_transform = A.Compose([\n",
    "        A.Normalize(mean=ADE_MEAN, std=ADE_STD)\n",
    "    ])\n",
    "\n",
    "    ### Get dataset\n",
    "    train_data_df = pd.read_csv(train_csv)\n",
    "    val_data_df = pd.read_csv(val_csv)\n",
    "    train_image_paths = train_data_df[csv_img_path_col].tolist()\n",
    "    train_mask_paths = train_data_df[csv_label_path_col].tolist()\n",
    "    val_image_paths = val_data_df[csv_img_path_col].tolist()\n",
    "    val_mask_paths = val_data_df[csv_label_path_col].tolist()\n",
    "\n",
    "    # Use custom dataset\n",
    "    train_dataset = ImageSegmentationDataset(train_image_paths, train_mask_paths, transform=train_transform)\n",
    "    val_dataset = ImageSegmentationDataset(val_image_paths, val_mask_paths, transform=val_transform)\n",
    "\n",
    "    # Create an empty preprocessor\n",
    "    preprocessor = MaskFormerImageProcessor(ignore_index=0, reduce_labels=False, do_resize=False, do_rescale=False, do_normalize=False)\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        inputs = list(zip(*batch))\n",
    "        images = inputs[0]\n",
    "        segmentation_maps = inputs[1]\n",
    "        # this function pads the inputs to the same size,\n",
    "        # and creates a pixel mask\n",
    "        # actually padding isn't required here since we are cropping\n",
    "        batch = preprocessor(\n",
    "            images,\n",
    "            segmentation_maps=segmentation_maps,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        batch[\"original_images\"] = inputs[2]\n",
    "        batch[\"original_segmentation_maps\"] = inputs[3]\n",
    "        \n",
    "        return batch\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=collate_fn)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=collate_fn)\n",
    "# Replaces the head of the pre-trained model\n",
    "    # model = MaskFormerForInstanceSegmentation.from_pretrained(\"facebook/maskformer-swin-base-ade\",\n",
    "    #                                                         id2label=id2label,\n",
    "    #                                                         ignore_mismatched_sizes=True)\n",
    "    model = Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-large-cityscapes-semantic\",\n",
    "                                                                id2label=id2label,\n",
    "                                                                ignore_mismatched_sizes=True)\n",
    "    model.to(device)\n",
    "\n",
    "    # experiment id should be a unique string value for the experiment. can use the output folders name\n",
    "    metric = evaluate.load(\"mean_iou\", experiment_id=output_directory.split('/')[-1] + str(time.time()))\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    print(f'begin train, len train: {len(train_dataloader)}, len val: {len(val_dataloader)}')\n",
    "\n",
    "    best_mean_iou = 0.0  # Track the best mean IoU\n",
    "    counter = 0  # Counter to monitor epochs since improvement\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        num_samples = 0\n",
    "\n",
    "        model.train()\n",
    "        for idx, batch in enumerate(train_dataloader):\n",
    "            # Reset the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                pixel_values=batch[\"pixel_values\"].to(device),\n",
    "                mask_labels=[labels.to(device) for labels in batch[\"mask_labels\"]],\n",
    "                class_labels=[labels.to(device) for labels in batch[\"class_labels\"]],\n",
    "            )\n",
    "\n",
    "            # Backward propagation\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            batch_size = batch[\"pixel_values\"].size(0)\n",
    "            running_loss += loss.item()\n",
    "            num_samples += batch_size\n",
    "\n",
    "            if idx % 1 == 0:\n",
    "                print(\"Loss: \", loss)\n",
    "\n",
    "            # Optimization\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = running_loss/num_samples\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        for idx, batch in enumerate(val_dataloader):\n",
    "            pixel_values = batch[\"pixel_values\"]\n",
    "\n",
    "            # Forward pass\n",
    "            with torch.no_grad():\n",
    "                outputs = model(pixel_values=pixel_values.to(device))\n",
    "\n",
    "            # get original images\n",
    "            original_images = batch[\"original_images\"]\n",
    "            target_sizes = [(image.shape[0], image.shape[1]) for image in original_images]\n",
    "            # predict segmentation maps\n",
    "            predicted_segmentation_maps = preprocessor.post_process_semantic_segmentation(outputs,\n",
    "                                                                                        target_sizes=target_sizes)\n",
    "            \n",
    "            if idx < num_val_outputs_to_save:\n",
    "                # Val\n",
    "                segmentation_map = predicted_segmentation_maps[0].cpu().numpy()\n",
    "\n",
    "                color_segmentation_map = np.zeros((segmentation_map.shape[0], segmentation_map.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
    "                for label, color in enumerate(palette):\n",
    "                    color_segmentation_map[segmentation_map - 1 == label, :] = color\n",
    "                # Convert to BGR\n",
    "                pred_color_seg = color_segmentation_map[..., ::1]\n",
    "\n",
    "                img = original_images[0] * 0.7 + pred_color_seg * 0.3\n",
    "                img = img.astype(np.uint8)\n",
    "\n",
    "                epoch_directory = os.path.join(inference_directory, f'{epoch}')\n",
    "                os.makedirs(epoch_directory, exist_ok=True)\n",
    "                channel_directory = os.path.join(epoch_directory, f'{idx}')\n",
    "                os.makedirs(channel_directory, exist_ok=True)\n",
    "                plt.figure(figsize=(15, 10))\n",
    "                plt.imshow(img)\n",
    "                plt.show()\n",
    "                plt.axis('off')\n",
    "                loss_str = str(outputs.loss)\n",
    "                loss_str = loss_str.replace('.', '_')\n",
    "                channel_filename = os.path.join(channel_directory, f'val_segmentation_loss{loss_str}.png')\n",
    "                plt.savefig(channel_filename, bbox_inches='tight')\n",
    "                plt.close()\n",
    "\n",
    "                # Ground truth\n",
    "                segmentation_map = batch[\"original_segmentation_maps\"][0]\n",
    "\n",
    "                color_segmentation_map = np.zeros((segmentation_map.shape[0], segmentation_map.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
    "                for label, color in enumerate(palette):\n",
    "                    color_segmentation_map[segmentation_map - 1 == label, :] = color\n",
    "                # Convert to BGR\n",
    "                ground_truth_color_seg = color_segmentation_map[..., ::1]\n",
    "\n",
    "                img = original_images[0] * 0.7 + ground_truth_color_seg * 0.3\n",
    "                img = img.astype(np.uint8)\n",
    "\n",
    "                plt.figure(figsize=(15, 10))\n",
    "                plt.imshow(img)\n",
    "                plt.show()\n",
    "                plt.axis('off')\n",
    "                channel_filename = os.path.join(channel_directory, f'gt_segmentation.png')\n",
    "                plt.savefig(channel_filename, bbox_inches='tight')\n",
    "                plt.close()\n",
    "\n",
    "            # get ground truth segmentation maps\n",
    "            ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"]\n",
    "\n",
    "            metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)\n",
    "\n",
    "        # Step the scheduler to update the learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # NOTE this metric outputs a dict that also includes the mIoU per category as keys\n",
    "        # so if you're interested, feel free to print them as well\n",
    "        mean_iou = metric.compute(num_labels=len(id2label), ignore_index=0)['mean_iou']\n",
    "        val_losses.append(mean_iou)\n",
    "        print(\"Mean IoU:\", mean_iou)\n",
    "        \n",
    "        # Save the model if mean IoU improves\n",
    "        if mean_iou > best_mean_iou:\n",
    "            best_mean_iou = mean_iou\n",
    "            counter = 0  # Reset the counter\n",
    "            model_filename = os.path.join(model_directory, f'model_epoch{epoch}_ckpt.pt')\n",
    "            torch.save(model.state_dict(), model_filename)  # Save the model\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        # Check for early stopping based on patience\n",
    "        if counter >= patience:\n",
    "            print(f\"No improvement for {patience} epochs. Early stopping.\")\n",
    "            break\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.plot(range(1, epoch+2), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, epoch+2), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss and Validation Mean IOU Curves')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(output_directory, 'loss_curves.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def split_csv_by_dataset(input_csv_path, output_folder):\n",
    "    # Read the combined CSV file\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "\n",
    "    # Check if output directory exists, if not create it\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Group by dataset name and write separate CSV files\n",
    "    for dataset_name, group in df.groupby('dataset_name'):\n",
    "        output_csv_path = os.path.join(output_folder, f'{dataset_name}_data.csv')\n",
    "        group.to_csv(output_csv_path, index=False)\n",
    "        print(f\"CSV file for {dataset_name} saved as {output_csv_path}\")\n",
    "\n",
    "# Example usage\n",
    "input_csv_path = '/home/thakuriu/fl_glaucoma_seg/csvs/combined_train.csv'\n",
    "output_folder = '/home/thakuriu/fl_glaucoma_seg/csvs/split_datasets'\n",
    "split_csv_by_dataset(input_csv_path, output_folder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
